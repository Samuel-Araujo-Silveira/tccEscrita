
%VERIFICAR SE HÁ UMA PADRONIZAÇÃO DE DISTANCIA.

%

Neste capítulo, serão apresentadas todas as ferramentas e os procedimentos utilizados para a construção do modelo computacional de reconhecimento e classificação oócitos bovinos.

Primeiramente, os oócitos foram visualizados através de instrumentos de aproximação por conta de seu tamanho microscópico e as suas imagens foram capturadas e armazenadas. Em seguida, as imagens captadas são pré-processadas para a retirada de ruídos e padronização de camadas; processadas em uma rede neural para reconhecer as regiões onde se encontram os oócitos; construção do modelo para classificá-los; e, finalmente, um modelo final que une essas duas funcionalidades. Esse processo pode ser visualizado na figura \ref{fig:processo} e será descrito com mais detalhes nas seções seguintes.

\begin{figure}[H]
	\centering
	\caption{Demonstração das Etapas de Construção do Modelo}
	\includegraphics[width=0.9\textwidth]{images/ia/processo6.png}
	\caption*{\textbf{Fonte: Elaboração própria}}
	\label{fig:processo}
\end{figure}

\section{Visualização e Coleta}

Os oócitos utilizadas neste trabalho foram fornecidas pela empresa parceira BioInova \cite{bioinnova2024}. Ela possui experiência com produções \textit{in vitro} desde 2019 e, por conta disso, possui um vasto \textit{dataset} de oócitos que será necessário para realizar o processo de aprendizado do sistema .

A visualização e captura das imagens são feitas utilizando um instrumento de aproximação microscópico. Ele é demonstrado na Figura \ref{fig:lupa}

\begin{figure}[H]
	\centering
	\caption{Demonstração das componentes de um Microscópio Estereoscópio}
	\includegraphics[width=0.8\textwidth]{images/ferramentas/lupa.png}
	\caption*{\textbf{Fonte: \citeonline{gomes2005}.}} 
	\label{fig:lupa}
\end{figure}

Ele é equipado com uma câmera, o que permite a coletada das imagens de maneira apropriada. Essa é composta por um conjunto óptico de três câmeras, disponíveis num celular Samsung Galaxy s20 fe, que segundo o fabricante, é composta  12, 64 e 12 megapixels (MP) e aberturas de f1.8, f2.0 e f2.2, respectivamente (SAMSUNG, 2024).

\section{Pré-processamento}

Nessa seção, todas as etapas necessárias para preparar as condições mínicas necessárias para o desenvolvimento do modelo de identificação e classificação de oócitos serão abordadas. 

\subsection{OpenCV}
O pré-processamento consiste em corrigir aspectos ruidosos nas imagens, tais como: sujeiras, áreas ofuscadas, dentre outros. Essas correções podem ser radiométricas ou geométricas \cite{CHAKI2019} e são realizadas através da biblioteca OpenCV e seus métodos.

Algumas imagens passaram por uma correção radiométrica \cite{CHAKI2019}. Esse tipo de problema surge quando há um mal posicionamento do objeto capturado diante da luz ou falta de calibração adequada nos sensores. Isso faz com que alguns \textit{pixels} da imagem não sejam constituídos, ou seja, será necessário reconstituí-los de forma artificial através dos \textit{pixels} mais próximos como referência \cite{CHAKI2019}. Para tratar esse tipo de problema, os fundamentos utilizados são equalização de histograma e conversão de espaço de cores\ \cite{his} . 

Houve também casos em que movimentos durante o processo de captura e/ou lentes distorcidas no dispositivo de captação ocasionam um posicionamento distorcido de determinados \textit{pixels} da imagem \cite{CHAKI2019}. Para resolver isso, foi necessário reposicionar os \textit{pixels} que foram desvirtuados através das técnicas de transformações geométrica da OpenCV, que são: correção de distorção de lente, transformação de perspectiva, e transformação afim \cite{geo}. 

Para ajustes na iluminação, utilizou-se filtros de gradiente e equalizações de histograma, que servem para calibrar a regularidade de intensidade de \textit{pixels} \cite{KRIG2014}. 

Por fim, houve a necessidade de realizar ajustes relacionados ao foco de determinadas regiões da imagem captada. Por exemplo: se alguma região de interesse estiver desfocada ou se alguma região irrelevante estiver com muito foco, ambas essas situações precisarão ser reajustadas para que o processamento ocorra adequadamente \cite{KRIG2014}. 

\subsection{LabelMe}
O \textit{LabelMe} foi utilizado para rotulação \cite{russell2008labelme}. Um círculo foi atribuído em cada oócito para fazer com que o aprendizado identificasse qual é o objeto de aprendizado. Logo depois, um JSON é gerado com todas as informações pertinentes da rotulação, no entanto, ainda foi necessário usar o comando \textit{labelme2coco}, que traduz o JSON gerado para o formato esperado pelo Detectron2. 




\section{Construção do Modelo de Identificação}
Com o conjunto de imagens completamente pré-processado e rotulado, foi possível utilizar o Detectron2, que de acordo com \cite{Lad2024Detectron2}, é um \textit{framework} para diversos algoritmos de detecção de objetos e, entre eles, encontra-se o Mask R-CNN.  

Logo após, um modelo de aprendizado de máquina para identificar oócitos bovinos foi gerado com o Detectron2. A arquitetura desse modelo está representada na Figura \ref{fig:arquitetura}

\begin{figure}[H]
	\centering
	\caption{Arquitetura do Modelo de Aprendizado de Máquina para Identificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/arquitetura2.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:arquitetura}
\end{figure}

Percebe-se uma arquitetura baseada no \textit{Mask R-CNN} utilizado pelo \textit{Detectron2} e adaptada especificamente para detecção e segmentação de oócitos bovinos.

Essa rede é composta por três módulos principais:


\begin{itemize}
	\item \textbf{Backbone (ResNet + FPN)}: as imagens dos oócitos foram processadas e suas características, extraídas. A \textit{Feature Pyramid Network} (FPN) foi responsável por combinar informações em múltiplas escalas (fpnlateral2 – 5) com convoluções de 256 canais, algo que permitiu detectar oócitos de diferentes tamanhos e contrastes;
	
	\item \textbf{\textit{Region Proposal Network} (RPN)}: aqui, regiões de interesse foram geradas através de probabiliades de cada região conter um objeto de interesse;
	
	\item \textbf{ROI Heads (Box Head e Mask Head)}: as propostas geradas na etapa anterior foram refinadas através de classificações e ajustes das caixas delimitadoras dos oócitos.
	
\end{itemize}

Logo após, houve a construção do modelo de identificação com os conjuntos de nome \textit{oocitosTrain} e \textit{oocitosVal} configurados para 1 classe (oócito) e com 1000 iterações.

Com o modelo para identificar oócitos treinado, tornou-se possível desenvolver a etapa de classificação. Pode-se averiguar o funcionamento de modelo de identificação na Figura \ref{fig:entradaEsaida}

\begin{figure}[H]
	\centering
	\caption{Imagem de Entrada e Saída no Modelo de Identificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/saida2.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:entradaEsaida}
\end{figure}

Para que esse modelo pudesse ser treinamento corretamente, foi utilizada uma proporção de 80\% da base de dados para treinamento e 20\% para validação, que é uma divisão padrão.



Após a finalização dessa etapa, tornou-se necessário avaliar a eficiência do modelo de identificação. Com esse intuito, o Detectron2 utiliza o método \textit{COCOEvaluator}.


\section{Construção do Modelo de Classificação}
Para a construção do modelo de classificação foi necessário isolar através de recortes os oócitos contidos nas imagens. Isso foi feito para facilitar o aprendizado das características específicas de cada tipo. Em seguida, a partir do VGG Image Annotator (VIA), 4 rótulos foram criados e todos os objetos de interesse isolados foram atribuídos à sua respectiva divisão. Nesse sentido, foi possível gerar um JSON no formato adequado para o Detectron2, algo que o VIA gera automaticamente . Entretanto, o algoritmo \textit{cocosplit.py} ainda foi importante para dividir o JSON gerado em outros dois JSONS: um para treinamento e outro para teste a partir da proporção de 80\% para 20\%. Para garantir que cada classe fosse bem representada nas duas divisões, o \textit{cocossplit.py} precisou ser modificado para realizar proporções inteligentes para cada classificação tendo em vista que seu código original não faz essa diferenciação. 

Logo após, o modelo de aprendizado de máquina para classificação de oócitos bovinos foi construído. Sua arquitetura pode ser visualizada na Figura \ref{fig:arquitetura2}

\begin{figure}[H]
	\centering
	\caption{Arquitetura do Modelo de Aprendizado de Máquina para Classificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/arquitetura6.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:arquitetura2}
\end{figure}

Como está representado na imagem, o oócito isolado foi processado por uma série de camada convolucionais na etapa de \textit{backbone}. Esse processo extrai mapas de características da classificação.

Em seguida, as informações pertinentes são extraídas pelo \textit{backbone} são processados pela \textit{Feature Pyramid Network} (FPN). Essa etapa combina mapas de características de diferentes resoluções. Isso serve para melhorar a capacidade do modelo em detectar objetos de tamanhos variados.

Após a extração e combinação das características, os mapas gerados pela FPN são enviados à Region Proposal Network (RPN). Ela serve para gerar regiões com probabilidade de obter objetos de interesse, ou seja, oócitos bovinos de determinada classificação. Além do mais, ela analisa as áreas de interesse e ajusta as coordenadas das (\textit{bounding boxes}).

Antes das regiões propostas serem enviadas para as etapas de \textit{bounding box regressor} e \textit{box classifier}, há um procedimento intermediário executado pelo \textit{ROI Align}, que serve para reginar as coordenadas que classificam os oócitos.

Logo após, a \textit{bounding box regressor} realiza a atribuição de rótulos de classificação aos oócitos com determinadas características e refina ainda mais as coordenadas. 

A partir da finalização desse processo, o modelo é capaz de produzir três tipos de resultados: a classificação de cada objeto de interesse, as caixas delimitadoras e as máscaras de segmentação, que contornam cada pixel da instância.

Com o modelo treinado, cada oócito isolado passou a ser classificado. Pode-se visualizar essa etapa na Figura \ref{fig:visualizacao}

\begin{figure}[H]
	\centering
	\caption{Demonstração da classificação de um oócito bovino}
	\includegraphics[width=0.8\textwidth]{images/ia/visualizacao.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:visualizacao}
\end{figure}


 \section{Modelo Final}
 Após o treinamento dos dois modelos, tornou-se necessário desenvolver um algoritmo através da linguagem de programação \textit{Python} que une as funcionalidades de identificação e classificão. Primeiramente, ele carrega as funções pertinentes dos dois modelos que estão nos segunuintes arquivos: model\_final\_identificacao.pth, config\_identifacao.yaml, model\_final\_classificacao.pth e config\_classificacao.yaml.
 
 A partir desses dados, foi desenvolvida uma função que analisa toas as imagens dentro um diretório nominado de \textit{input} e utiliza o modelo de identificação para recortar os oócitos de imagens que possuem vários objetos de interesse. Logo após, as imagens de cada oócito isolado são armazenadas em uma pasta chamada de intermédio. Com isso, outra função é chamada para classificar cada instância e, no fim do processo, diretórios referentes a cada classificação são criados dentro de uma pasta chamada de \textit{output}. Em seguida, cada óocito classificado é armazenado de acordo com seu diretório. 
 
Todo esse processo pode ser visualizado na Figura \ref{fig:modeloFinal}

\begin{figure}[H]
	\centering
	\caption{Demonstração do funcionamento do Modelo Final}
	\includegraphics[width=0.8\textwidth]{images/ia/modeloFinal.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:modeloFinal}
\end{figure}

 
 


