
%VERIFICAR SE HÁ UMA PADRONIZAÇÃO DE DISTANCIA.

%

Neste capítulo, serão apresentadas todas as ferramentas e os procedimentos utilizados para a construção do modelo computacional de reconhecimento e classificação oócitos bovinos.

No modelo proposto, os oócitos são visualizados através de instrumentos de aproximação por conta de seu tamanho microscópico e as suas imagens são capturadas e armazenadas. Em seguida, as imagens captadas são pré-processadas para a retirada de ruídos e padronização de camadas; processadas em uma rede neural para reconhecer as regiões onde se encontram os oócitos; construção do modelo para classificá-los; e, finalmente, um modelo final que une essas duas funcionalidades. Esse processo pode ser visualizado na figura \ref{fig:processo} e será descrito com mais detalhes nas seções seguintes.

\begin{figure}[H]
	\centering
	\caption{Esquema de funcionamento do modelo proposto}
	\includegraphics[width=0.9\textwidth]{images/ia/processo6.png}
	\caption*{\textbf{Fonte: Elaboração própria}}
	\label{fig:processo}
\end{figure}

\section{Visualização e Coleta}

Os oócitos utilizadas neste trabalho foram fornecidas pela empresa parceira BioInova \cite{bioinnova2024}. Ela possui experiência com produções \textit{in vitro} desde 2019 e, por conta disso, possui um vasto \textit{dataset} de oócitos que será necessário para realizar o processo de aprendizado do sistema .

A visualização e captura das imagens são feitas utilizando um instrumento de aproximação microscópico. Ele é demonstrado na Figura \ref{fig:lupa}

\begin{figure}[H]
	\centering
	\caption{Demonstração das componentes de um Microscópio Estereoscópio}
	\includegraphics[width=0.8\textwidth]{images/ferramentas/lupa.png}
	\caption*{\textbf{Fonte: \citeonline{gomes2005}.}} 
	\label{fig:lupa}
\end{figure}

Ele é equipado com uma câmera, o que permite a coletada das imagens de maneira apropriada. Essa é composta por um conjunto óptico de três câmeras, disponíveis num celular Samsung Galaxy s20 fe, que segundo o fabricante, é composta  12, 64 e 12 megapixels (MP) e aberturas de f1.8, f2.0 e f2.2, respectivamente (SAMSUNG, 2024).

\section{Pré-processamento}

Nessa seção, todas as etapas necessárias para preparar as condições mínicas necessárias para o desenvolvimento do modelo de identificação e classificação de oócitos serão abordadas. 

\subsection{OpenCV}
O pré-processamento consiste em corrigir aspectos ruidosos nas imagens, tais como: sujeiras, áreas ofuscadas, dentre outros. Essas correções podem ser radiométricas ou geométricas \cite{CHAKI2019} e são realizadas através da biblioteca OpenCV e seus métodos.

Algumas imagens passaram por uma correção radiométrica \cite{CHAKI2019}. Esse tipo de problema surge quando há um mal posicionamento do objeto capturado diante da luz ou falta de calibração adequada nos sensores. Isso faz com que alguns \textit{pixels} da imagem não sejam constituídos, ou seja, será necessário reconstituí-los de forma artificial através dos \textit{pixels} mais próximos como referência \cite{CHAKI2019}. Para tratar esse tipo de problema, os fundamentos utilizados são equalização de histograma e conversão de espaço de cores\ \cite{OPENCV2024} . 

Houve também casos em que movimentos durante o processo de captura e/ou lentes distorcidas no dispositivo de captação ocasionam um posicionamento distorcido de determinados \textit{pixels} da imagem \cite{CHAKI2019}. Para resolver isso, foi necessário reposicionar os \textit{pixels} que foram desvirtuados através das técnicas de transformações geométrica da OpenCV, que são: correção de distorção de lente, transformação de perspectiva, e transformação afim \cite{OPENCV2024}. 

Para ajustes na iluminação, utilizou-se filtros de gradiente e equalizações de histograma, que servem para calibrar a regularidade de intensidade de \textit{pixels} \cite{KRIG2014}. 

Por fim, houve a necessidade de realizar ajustes relacionados ao foco de determinadas regiões da imagem captada. Por exemplo: se alguma região de interesse estiver desfocada ou se alguma região irrelevante estiver com muito foco, ambas essas situações precisarão ser reajustadas para que o processamento ocorra adequadamente \cite{KRIG2014}. 

\subsection{LabelMe}
O \textit{LabelMe} foi utilizado para rotulação \cite{russell2008labelme}. Um círculo foi atribuído em cada oócito para fazer com que o aprendizado identificasse qual é o objeto de aprendizado. Logo depois, um JSON é gerado com todas as informações pertinentes da rotulação, no entanto, ainda foi necessário usar o comando \textit{labelme2coco}, que traduz o JSON gerado para o formato esperado pelo Detectron2, a CNN utilizada na tarefa de identificação das células e que falaremos mais na seção 5.3. 




\section{Construção do Modelo de Identificação}
Com o conjunto de imagens completamente pré-processado e rotulado, foi possível utilizar o Detectron2, que de acordo com \cite{Lad2024Detectron2}, é um \textit{framework} para diversos algoritmos de detecção de objetos e, entre eles, encontra-se o Mask R-CNN.  

Logo após, um modelo de aprendizado de máquina para identificar oócitos bovinos foi gerado com o Detectron2. A arquitetura desse modelo está representada na Figura \ref{fig:arquitetura}

\begin{figure}[H]
	\centering
	\caption{Arquitetura do Modelo de Aprendizado de Máquina para Identificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/arquitetura2.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:arquitetura}
\end{figure}

Percebe-se uma arquitetura baseada no \textit{Mask R-CNN} utilizado pelo \textit{Detectron2} e adaptada especificamente para detecção e segmentação de oócitos bovinos.

Essa rede é composta por três módulos principais:


\begin{itemize}
	\item \textbf{Backbone (ResNet + FPN)}: as imagens dos oócitos foram processadas e suas características, extraídas. A \textit{Feature Pyramid Network} (FPN) foi responsável por combinar informações em múltiplas escalas (fpnlateral2 – 5) com convoluções de 256 canais;
	
	\item \textbf{\textit{Region Proposal Network} (RPN)}: aqui, regiões de interesse foram geradas através de probabiliades de cada região conter um objeto de interesse;
	
	\item \textbf{ROI Heads (Box Head e Mask Head)}: as propostas geradas na etapa anterior foram refinadas através de classificações e ajustes das caixas delimitadoras dos oócitos.
	
	\item \textbf{Conv2d}: são as camadas convolucionais cujos filtros varrem as imagens e procuram características;
	
	\item \textbf{Flatten}: uma camada do processo responsável por converter matrizes em vetores com o intuito de possibilitar a entrada dessas informações em camadas totalmente conectadas;
	
\end{itemize}

Baseado nessa arquitetura, houve a construção do modelo de identificação com os conjuntos de nome \textit{oocitosTrain} e \textit{oocitosVal} que servem, respectivamente, para treinar o modelo e validar o treinamento realizado com testes. Eles foram configurados para 1 classe (oócito) e com 1000 iterações.

Com o modelo para identificar oócitos treinado, o próximo passo é desenvolver a etapa de classificação. Pode-se averiguar o funcionamento de modelo de identificação na Figura \ref{fig:entradaEsaida}

\begin{figure}[H]
	\centering
	\caption{Imagem de Entrada e Saída no Modelo de Identificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/saida2.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:entradaEsaida}
\end{figure}

Para que esse modelo pudesse ser treinado e validado corretamente, foi utilizada uma proporção de 80\% da base de dados para treinamento e 20\% para validação, que é uma divisão padrão.



Após a finalização dessa etapa, tornou-se necessário avaliar a eficiência do modelo de identificação. Com esse intuito, o Detectron2 utiliza o método \textit{COCOEvaluator}.


\section{Construção do Modelo de Classificação}
Para a construção do modelo de classificação foi necessário isolar através de recortes os oócitos contidos nas imagens. Isso foi feito para facilitar o aprendizado das características específicas de cada tipo. Em seguida, a partir do VGG Image Annotator\footnote{VGG Image Annotator é um \textit{software} para rotulação de imagens de \textit{datasets}}  (VIA), 4 rótulos foram criados e todos os objetos de interesse isolados foram atribuídos à sua respectiva divisão. Nesse sentido, foi possível gerar um JSON no formato adequado para o Detectron2, algo que o VIA gera automaticamente . Entretanto, o algoritmo \textit{cocosplit} ainda foi importante para dividir o JSON gerado em outros dois JSONs: um para treinamento e outro para teste a partir da proporção de 80\% para 20\%. Para garantir que cada classe fosse bem representada nas duas divisões, o \textit{cocossplit.py} precisou ser modificado para realizar proporções inteligentes para cada classificação tendo em vista que seu código original não faz essa diferenciação.



Com essas condições e características, o modelo de aprendizado de máquina para classificação de oócitos bovinos foi construído. Sua arquitetura pode ser visualizada na Figura \ref{fig:arquitetura2}

\begin{figure}[H]
	\centering
	\caption{Arquitetura do Modelo de Aprendizado de Máquina para Classificação de Oócitos}
	\includegraphics[width=0.8\textwidth]{images/ia/procedi2.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:arquitetura2}
\end{figure}

Como está representado na imagem, o oócito isolado foi processado por uma série de camada convolucionais na etapa de rede base de extração de características. Esse processo extrai mapas com informações pertinentes para a classificação.

Em seguida, essas informações são processados pela rede de pirâmide de características. Essa etapa combina mapas de características de diferentes resoluções. Isso serve para melhorar a capacidade do modelo em detectar objetos de tamanhos variados.

Após a extração e combinação das características, os mapas gerados pela rede de pirâmide são enviados à rede propositora de regiões. Ela serve para gerar regiões com probabilidade de obter objetos de interesse, ou seja, oócitos bovinos de determinada classificação. Além do mais, ela analisa as áreas de interesse e ajusta as coordenadas das caixas delimitadoras.

Antes das regiões propostas serem enviadas para as etapas de regressor de caixa delimitadora e classificador de caixa, há um procedimento intermediário executado pelo alinhamento de região de interesse, que serve para reajustar as coordenadas que classificam os oócitos.

Logo após,o regressor de caixa delimitadora realiza a atribuição de rótulos de classificação aos oócitos com determinadas características e refina ainda mais as coordenadas. 

A partir da finalização desse processo, o modelo através das caixas delimitadoras e máscaras de segmentação é capaz de produzir o resultado esperado: a classificação de cada objeto de interesse.

Com o modelo treinado, cada oócito isolado passou a ser classificado. Pode-se visualizar essa etapa na Figura \ref{fig:visualizacao}

\begin{figure}[H]
	\centering
	\caption{Demonstração da classificação de um oócito bovino}
	\includegraphics[width=0.8\textwidth]{images/ia/visualizacao.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:visualizacao}
\end{figure}

\subsection{Tentativas de Aprimoramento do Modelo de Classificação}

Após o treinamento do modelo, tornou-se necessário aplicar algumas técnicas para aprimorá-lo por conta do problema de \textit{overfitting} que foi encontrado. Elas são:

\begin{itemize}
	\item \textbf{Aumento do \textit{Dataset}}: a primeira versão do modelo foi treinada com, em média, 16 oócitos para classificação. Esse valor foi escolhido porque a classificação 3 de oócitos, que foi a mais difícil de achar, continha apenas 16 oócitos em todo o \textit{dataset}, enquanto as outras classificações possuíam consideravelmente mais imagens. Como o \textit{overfitting} aconteceu, foi decidido alterar o balanceamento e utilizar o total de imagens que cada classe possuía para si;
	
	\item \textbf{Regularização L2}: a variável \textit{Weight Decay} presente na configuração do treinamento foi diminuída com o intuito de aumentar a generalização do aprendizado, tendo em vista que um grande valor torna o treinamento muito sensível para variações entre diferentes imagens de oócitos;
	
	\item \textbf{Regularização de Lote}: diminuir a variável \textit{BATCH\_SIZE\_PER\_IMAGE} força o treinamento a se adaptar para características mais amplas e combater a memorização das imagens de validação;
	
	\item \textbf{Otimização de Treinamento}: reduzir a variável \textit{BASE\_LR} serve pra estabilizar o aprendizado e diminuir a intensidade que o modelo se corrige durante o processo de treinamento;
	
	\item \textbf{Parada/Controle}: essa estratégia permite manipular a duração do treinamento. Ele verifica a partir de qual ponto o \textit{overfitting} começa através do monitoramento da variável AP e interrompe o treinamento assim que o problema de memorização começa.
\end{itemize}

 \section{Modelo Final}
 Após o treinamento dos dois modelos, tornou-se necessário desenvolver um algoritmo através da linguagem de programação \textit{Python} que une as funcionalidades de identificação e classificão. Primeiramente, ele carrega os arquivos dos próprios modelos e depois, carrega os arquivos que contém as estruturas de cada modelo.
 
 A partir desses dados, foi desenvolvida uma função que analisa todas as imagens dentro um diretório nominado de \textit{input} e utiliza o modelo de identificação para recortar os oócitos de imagens que possuem vários objetos de interesse. Logo após, as imagens de cada oócito isolado são armazenadas em uma pasta chamada de intermédio. Com isso, outra função é chamada para classificar cada instância e, no fim do processo, diretórios referentes a cada classificação são criados dentro de uma pasta chamada de \textit{output}. Em seguida, cada óocito classificado é armazenado de acordo com seu respectivo diretório. 
 
Todo esse processo pode ser visualizado na Figura \ref{fig:modeloFinal}

\begin{figure}[H]
	\centering
	\caption{Demonstração do funcionamento do Modelo Final}
	\includegraphics[width=0.8\textwidth]{images/ia/modeloFinal.png}
	\caption*{\textbf{Fonte: Elaboração Própria.}} 
	\label{fig:modeloFinal}
\end{figure}

 
 


